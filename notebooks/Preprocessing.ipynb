{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2e20ed-5da3-462c-b50c-39c7c05f8cb3",
   "metadata": {},
   "source": [
    "# Fundus Preprocessing Pipeline — Explanation\n",
    "\n",
    "## What this script does\n",
    "Takes a dataset in `dataset/<class_name>/<image>` format, **preprocesses fundus images**, saves 224×224 outputs to `preprocessed224_best/<class_name>/...`, and writes a `manifest.csv` with per-image status and label. It runs **multi-threaded** for speed.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs & knobs\n",
    "- **Paths:** `input_dir=\"dataset\"`, `output_dir=\"preprocessed224_best\"`\n",
    "- **Output size:** `size=224` (change here to produce e.g., 384)\n",
    "- **Workers:** `workers=8` (threads for parallel processing)\n",
    "- **Format:** `ext=\"jpg\"` (can be `png` etc.)\n",
    "- **Manifest:** `manifest.csv` in the output root\n",
    "\n",
    "---\n",
    "\n",
    "## File I/O helpers (robust to unicode paths)\n",
    "- `imread_any(path)`: reads bytes → `cv2.imdecode` (safer than `cv2.imread` for odd paths)\n",
    "- `imwrite_any(path, img, ext)`: encodes via `cv2.imencode` then writes bytes (controls JPEG quality/PNG compression)\n",
    "- Supports extensions: `.jpg, .jpeg, .png, .bmp, .tif, .tiff, .webp`\n",
    "\n",
    "---\n",
    "\n",
    "## Core preprocessing steps (fundus-specific)\n",
    "All steps occur in **`preprocess_fundus(img_bgr, size)`** in this order:\n",
    "\n",
    "1. **ROI crop (`fundus_roi_crop`)**  \n",
    "   - Threshold low intensities (`gray > 8`) to get a circular mask  \n",
    "   - Morphological close (7×7) to fill holes  \n",
    "   - **`robust_bbox_from_mask`**: bounding box with small margin → crop tightly to fundus\n",
    "\n",
    "2. **Illumination/shade correction (`shade_correction`)**  \n",
    "   - Divide by heavy Gaussian blur (σ≈40) to remove vignetting/shading; rescale\n",
    "\n",
    "3. **Color constancy (`shades_of_gray_cc`, p=6)**  \n",
    "   - Normalizes per-channel gains to reduce color cast\n",
    "\n",
    "4. **Local contrast on green (`clahe_on_green`)**  \n",
    "   - CLAHE on the **green channel** (vessels/lesions are prominent in G)\n",
    "\n",
    "5. **Optional global contrast (`optional_l_channel_clahe`)**  \n",
    "   - CLAHE on **L (lightness)** in LAB space for overall contrast\n",
    "\n",
    "6. **Adaptive gamma (`adaptive_gamma`, target=0.42)**  \n",
    "   - Computes image median brightness → sets gamma to reach target midtone\n",
    "\n",
    "7. **Sharpen (`unsharp`, σ=1.0, amount=0.5)**  \n",
    "   - Unsharp masking for detail enhancement\n",
    "\n",
    "8. **Square letterbox + resize (`letterbox_square`)**  \n",
    "   - Pads to square (no aspect distortion) and resizes to **224×224** (cubic)\n",
    "\n",
    "> Each step returns `uint8` BGR for OpenCV.\n",
    "\n",
    "---\n",
    "\n",
    "## Parallel processing\n",
    "- Collects all image files via `rglob`\n",
    "- Uses `ThreadPoolExecutor(workers)` to **process images concurrently**\n",
    "- Per-file function: **`process_one`**\n",
    "  - Skips if output already exists\n",
    "  - Reads → `preprocess_fundus` → writes → returns status (`ok`, `skipped`, `read_error`, `write_error`, or `error:<msg>`)\n",
    "\n",
    "---\n",
    "\n",
    "## Manifest & labels\n",
    "- Aggregates results into a **DataFrame (`df`)**\n",
    "- Infers **label** from first path component under `input_dir` (the class folder)\n",
    "- Saves `manifest.csv` with columns: `in`, `out`, `status`, `label`\n",
    "- Prints a **status summary** at the end\n",
    "\n",
    "---\n",
    "\n",
    "## Things you can tune quickly\n",
    "- **`size`** (e.g., 384) and **`output_dir`**\n",
    "- CLAHE params: `clip`, `tile`\n",
    "- **Gamma target** (`adaptive_gamma(target=...)`)\n",
    "- **Shade σ** (`shade_correction(sigma=...)`)\n",
    "- **Unsharp** (`sigma`, `amount`)\n",
    "- **Margin** in `robust_bbox_from_mask(margin_ratio=...)`\n",
    "- **`workers`** for faster preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d82d8a7d-db15-48f8-972c-90abd1cbf983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing (fundus best): 100%|█████████████████████████████████████████████████| 4728/4728 [04:08<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " status\n",
      "ok    4728\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved images & manifest to: preprocessed224_best\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== SET YOUR PATHS HERE ======\n",
    "input_dir  = r\"dataset\"        # folder with class folders\n",
    "output_dir = r\"preprocessed224_best\"\n",
    "size       = 224\n",
    "ext        = \"jpg\"\n",
    "workers    = 8\n",
    "manifest   = \"manifest.csv\"\n",
    "# =================================\n",
    "\n",
    "# --- all helper functions from before ---\n",
    "def imread_any(path: Path):\n",
    "    arr = np.fromfile(str(path), dtype=np.uint8)\n",
    "    return cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "def imwrite_any(path: Path, img_bgr: np.ndarray, ext=\"jpg\", jpg_quality=95, png_compress=3):\n",
    "    path = path.with_suffix(f\".{ext.lower()}\")\n",
    "    if ext.lower() in (\"jpg\", \"jpeg\"):\n",
    "        ok, enc = cv2.imencode(\".jpg\", img_bgr, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])\n",
    "    elif ext.lower() == \"png\":\n",
    "        ok, enc = cv2.imencode(\".png\", img_bgr, [int(cv2.IMWRITE_PNG_COMPRESSION), png_compress])\n",
    "    else:\n",
    "        ok, enc = cv2.imencode(f\".{ext}\", img_bgr)\n",
    "    if not ok:\n",
    "        return False\n",
    "    enc.tofile(str(path))\n",
    "    return True\n",
    "\n",
    "def robust_bbox_from_mask(mask: np.ndarray, margin_ratio=0.02):\n",
    "    coords = cv2.findNonZero(mask)\n",
    "    if coords is None:\n",
    "        h, w = mask.shape[:2]\n",
    "        return 0, 0, w, h\n",
    "    x, y, w, h = cv2.boundingRect(coords)\n",
    "    m = int(max(h, w) * margin_ratio)\n",
    "    x = max(0, x - m); y = max(0, y - m)\n",
    "    return x, y, min(mask.shape[1] - x, w + 2*m), min(mask.shape[0] - y, h + 2*m)\n",
    "\n",
    "def fundus_roi_crop(bgr: np.ndarray):\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 8, 255, cv2.THRESH_BINARY)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((7,7), np.uint8))\n",
    "    x, y, w, h = robust_bbox_from_mask(mask)\n",
    "    return bgr[y:y+h, x:x+w]\n",
    "\n",
    "def shade_correction(bgr: np.ndarray, sigma=40):\n",
    "    I = bgr.astype(np.float32) + 1.0\n",
    "    bg = cv2.GaussianBlur(I, (0,0), sigmaX=sigma, sigmaY=sigma)\n",
    "    corrected = I / (bg + 1e-6) * 128.0\n",
    "    return np.clip(corrected, 0, 255).astype(np.uint8)\n",
    "\n",
    "def shades_of_gray_cc(bgr: np.ndarray, p=6, eps=1e-6):\n",
    "    I = bgr.astype(np.float32)\n",
    "    Rp = np.power(I[:,:,2], p).mean() ** (1.0/p)\n",
    "    Gp = np.power(I[:,:,1], p).mean() ** (1.0/p)\n",
    "    Bp = np.power(I[:,:,0], p).mean() ** (1.0/p)\n",
    "    scale = (Rp + Gp + Bp) / 3.0\n",
    "    R = I[:,:,2] * (scale / (Rp + eps))\n",
    "    G = I[:,:,1] * (scale / (Gp + eps))\n",
    "    B = I[:,:,0] * (scale / (Bp + eps))\n",
    "    out = np.stack([B, G, R], axis=2)\n",
    "    return np.clip(out, 0, 255).astype(np.uint8)\n",
    "\n",
    "def clahe_on_green(bgr: np.ndarray, clip=2.0, tile=(8,8)):\n",
    "    b, g, r = cv2.split(bgr)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
    "    g2 = clahe.apply(g)\n",
    "    return cv2.merge([b, g2, r])\n",
    "\n",
    "def optional_l_channel_clahe(bgr: np.ndarray, clip=1.5, tile=(8,8)):\n",
    "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
    "    L, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
    "    L2 = clahe.apply(L)\n",
    "    lab2 = cv2.merge([L2, a, b])\n",
    "    return cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def adaptive_gamma(bgr: np.ndarray, target=0.42):\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "    med = np.median(gray) / 255.0\n",
    "    med = np.clip(med, 1e-4, 0.999)\n",
    "    gamma = np.log(target) / np.log(med)\n",
    "    inv = 1.0 / np.clip(gamma, 0.2, 5.0)\n",
    "    lut = np.arange(256, dtype=np.float32) / 255.0\n",
    "    lut = np.power(lut, inv)\n",
    "    lut = np.clip(lut * 255.0, 0, 255).astype(np.uint8)\n",
    "    return cv2.LUT(bgr, lut)\n",
    "\n",
    "def unsharp(bgr: np.ndarray, sigma=1.0, amount=0.5):\n",
    "    blurred = cv2.GaussianBlur(bgr, (0,0), sigmaX=sigma, sigmaY=sigma)\n",
    "    sharp = cv2.addWeighted(bgr, 1 + amount, blurred, -amount, 0)\n",
    "    return np.clip(sharp, 0, 255).astype(np.uint8)\n",
    "\n",
    "def letterbox_square(bgr: np.ndarray, size=224, color=(0,0,0)):\n",
    "    h, w = bgr.shape[:2]\n",
    "    dim = max(h, w)\n",
    "    top = (dim - h) // 2\n",
    "    bottom = dim - h - top\n",
    "    left = (dim - w) // 2\n",
    "    right = dim - w - left\n",
    "    padded = cv2.copyMakeBorder(bgr, top, bottom, left, right,\n",
    "                                cv2.BORDER_CONSTANT, value=color)\n",
    "    return cv2.resize(padded, (size, size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def preprocess_fundus(img_bgr: np.ndarray, size=224):\n",
    "    img = fundus_roi_crop(img_bgr)\n",
    "    img = shade_correction(img, sigma=40)\n",
    "    img = shades_of_gray_cc(img, p=6)\n",
    "    img = clahe_on_green(img, clip=2.0, tile=(8,8))\n",
    "    img = optional_l_channel_clahe(img, clip=1.5, tile=(8,8))\n",
    "    img = adaptive_gamma(img, target=0.42)\n",
    "    img = unsharp(img, sigma=1.0, amount=0.5)\n",
    "    img = letterbox_square(img, size=size, color=(0,0,0))\n",
    "    return img\n",
    "\n",
    "def process_one(in_path: Path, out_root: Path, rel: Path):\n",
    "    out_path = (out_root / rel).with_suffix(f\".{ext}\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": \"skipped\"}\n",
    "    bgr = imread_any(in_path)\n",
    "    if bgr is None:\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": \"read_error\"}\n",
    "    try:\n",
    "        proc = preprocess_fundus(bgr, size=size)\n",
    "        ok = imwrite_any(out_path, proc, ext=ext)\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": \"ok\" if ok else \"write_error\"}\n",
    "    except Exception as e:\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": f\"error:{e}\"}\n",
    "\n",
    "# --- run preprocessing ---\n",
    "in_root = Path(input_dir)\n",
    "out_root = Path(output_dir)\n",
    "out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "files = [p for p in in_root.rglob(\"*\") if p.suffix.lower() in exts]\n",
    "\n",
    "rows = []\n",
    "with ThreadPoolExecutor(max_workers=workers) as ex_pool:\n",
    "    futures = []\n",
    "    for p in files:\n",
    "        rel = p.relative_to(in_root)\n",
    "        futures.append(ex_pool.submit(process_one, p, out_root, rel))\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Preprocessing (fundus best)\"):\n",
    "        rows.append(fut.result())\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "labels = []\n",
    "for r in df[\"in\"]:\n",
    "    rel = Path(r).relative_to(in_root)\n",
    "    labels.append(rel.parts[0] if len(rel.parts) > 1 else \"unknown\")\n",
    "df[\"label\"] = labels\n",
    "df.to_csv(out_root / manifest, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Summary:\\n\", df[\"status\"].value_counts())\n",
    "print(f\"\\nSaved images & manifest to: {out_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3de86-6e1f-4027-ad57-005936ba4b1b",
   "metadata": {},
   "source": [
    "## Preprocessing 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "492f762b-16f0-47b5-863e-62182ae4c039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing (fundus → 299): 100%|████████████████████████████████████████████████| 4728/4728 [04:44<00:00, 16.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " status\n",
      "ok    4728\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved images & manifest to: preprocessed299_inception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== SET YOUR PATHS HERE (InceptionV3) ======\n",
    "input_dir  = r\"dataset\"                 # folder with class folders\n",
    "output_dir = r\"preprocessed299_inception\"\n",
    "size       = 299                        # <-- InceptionV3 wants 299x299\n",
    "ext        = \"jpg\"\n",
    "workers    = 8\n",
    "manifest   = \"manifest.csv\"\n",
    "# ===============================================\n",
    "\n",
    "# --- helpers ---\n",
    "def imread_any(path: Path):\n",
    "    arr = np.fromfile(str(path), dtype=np.uint8)\n",
    "    return cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "def imwrite_any(path: Path, img_bgr: np.ndarray, ext=\"jpg\", jpg_quality=95, png_compress=3):\n",
    "    path = path.with_suffix(f\".{ext.lower()}\")\n",
    "    if ext.lower() in (\"jpg\", \"jpeg\"):\n",
    "        ok, enc = cv2.imencode(\".jpg\", img_bgr, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])\n",
    "    elif ext.lower() == \"png\":\n",
    "        ok, enc = cv2.imencode(\".png\", img_bgr, [int(cv2.IMWRITE_PNG_COMPRESSION), png_compress])\n",
    "    else:\n",
    "        ok, enc = cv2.imencode(f\".{ext}\", img_bgr)\n",
    "    if not ok:\n",
    "        return False\n",
    "    enc.tofile(str(path))\n",
    "    return True\n",
    "\n",
    "def robust_bbox_from_mask(mask: np.ndarray, margin_ratio=0.02):\n",
    "    coords = cv2.findNonZero(mask)\n",
    "    if coords is None:\n",
    "        h, w = mask.shape[:2]\n",
    "        return 0, 0, w, h\n",
    "    x, y, w, h = cv2.boundingRect(coords)\n",
    "    m = int(max(h, w) * margin_ratio)\n",
    "    x = max(0, x - m); y = max(0, y - m)\n",
    "    return x, y, min(mask.shape[1] - x, w + 2*m), min(mask.shape[0] - y, h + 2*m)\n",
    "\n",
    "def fundus_roi_crop(bgr: np.ndarray):\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 8, 255, cv2.THRESH_BINARY)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((7,7), np.uint8))\n",
    "    x, y, w, h = robust_bbox_from_mask(mask)\n",
    "    return bgr[y:y+h, x:x+w]\n",
    "\n",
    "def shade_correction(bgr: np.ndarray, sigma=40):\n",
    "    I = bgr.astype(np.float32) + 1.0\n",
    "    bg = cv2.GaussianBlur(I, (0,0), sigmaX=sigma, sigmaY=sigma)\n",
    "    corrected = I / (bg + 1e-6) * 128.0\n",
    "    return np.clip(corrected, 0, 255).astype(np.uint8)\n",
    "\n",
    "def shades_of_gray_cc(bgr: np.ndarray, p=6, eps=1e-6):\n",
    "    I = bgr.astype(np.float32)\n",
    "    Rp = np.power(I[:,:,2], p).mean() ** (1.0/p)\n",
    "    Gp = np.power(I[:,:,1], p).mean() ** (1.0/p)\n",
    "    Bp = np.power(I[:,:,0], p).mean() ** (1.0/p)\n",
    "    scale = (Rp + Gp + Bp) / 3.0\n",
    "    R = I[:,:,2] * (scale / (Rp + eps))\n",
    "    G = I[:,:,1] * (scale / (Gp + eps))\n",
    "    B = I[:,:,0] * (scale / (Bp + eps))\n",
    "    out = np.stack([B, G, R], axis=2)\n",
    "    return np.clip(out, 0, 255).astype(np.uint8)\n",
    "\n",
    "def clahe_on_green(bgr: np.ndarray, clip=2.0, tile=(8,8)):\n",
    "    b, g, r = cv2.split(bgr)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
    "    g2 = clahe.apply(g)\n",
    "    return cv2.merge([b, g2, r])\n",
    "\n",
    "def optional_l_channel_clahe(bgr: np.ndarray, clip=1.5, tile=(8,8)):\n",
    "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
    "    L, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
    "    L2 = clahe.apply(L)\n",
    "    lab2 = cv2.merge([L2, a, b])\n",
    "    return cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def adaptive_gamma(bgr: np.ndarray, target=0.42):\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "    med = np.median(gray) / 255.0\n",
    "    med = np.clip(med, 1e-4, 0.999)\n",
    "    gamma = np.log(target) / np.log(med)\n",
    "    inv = 1.0 / np.clip(gamma, 0.2, 5.0)\n",
    "    lut = np.arange(256, dtype=np.float32) / 255.0\n",
    "    lut = np.power(lut, inv)\n",
    "    lut = np.clip(lut * 255.0, 0, 255).astype(np.uint8)\n",
    "    return cv2.LUT(bgr, lut)\n",
    "\n",
    "def unsharp(bgr: np.ndarray, sigma=1.0, amount=0.5):\n",
    "    blurred = cv2.GaussianBlur(bgr, (0,0), sigmaX=sigma, sigmaY=sigma)\n",
    "    sharp = cv2.addWeighted(bgr, 1 + amount, blurred, -amount, 0)\n",
    "    return np.clip(sharp, 0, 255).astype(np.uint8)\n",
    "\n",
    "def letterbox_square(bgr: np.ndarray, size=299, color=(0,0,0)):\n",
    "    h, w = bgr.shape[:2]\n",
    "    dim = max(h, w)\n",
    "    top = (dim - h) // 2\n",
    "    bottom = dim - h - top\n",
    "    left = (dim - w) // 2\n",
    "    right = dim - w - left\n",
    "    padded = cv2.copyMakeBorder(bgr, top, bottom, left, right,\n",
    "                                cv2.BORDER_CONSTANT, value=color)\n",
    "    return cv2.resize(padded, (size, size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def preprocess_fundus(img_bgr: np.ndarray, size=299):\n",
    "    img = fundus_roi_crop(img_bgr)\n",
    "    img = shade_correction(img, sigma=40)\n",
    "    img = shades_of_gray_cc(img, p=6)\n",
    "    img = clahe_on_green(img, clip=2.0, tile=(8,8))\n",
    "    img = optional_l_channel_clahe(img, clip=1.5, tile=(8,8))\n",
    "    img = adaptive_gamma(img, target=0.42)\n",
    "    img = unsharp(img, sigma=1.0, amount=0.5)\n",
    "    img = letterbox_square(img, size=size, color=(0,0,0))  # -> 299x299\n",
    "    return img\n",
    "\n",
    "def process_one(in_path: Path, out_root: Path, rel: Path):\n",
    "    out_path = (out_root / rel).with_suffix(f\".{ext}\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": \"skipped\"}\n",
    "    bgr = imread_any(in_path)\n",
    "    if bgr is None:\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": \"read_error\"}\n",
    "    try:\n",
    "        proc = preprocess_fundus(bgr, size=size)\n",
    "        ok = imwrite_any(out_path, proc, ext=ext)\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": \"ok\" if ok else \"write_error\"}\n",
    "    except Exception as e:\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": f\"error:{e}\"}\n",
    "\n",
    "# --- run preprocessing ---\n",
    "in_root = Path(input_dir)\n",
    "out_root = Path(output_dir)\n",
    "out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "files = [p for p in in_root.rglob(\"*\") if p.suffix.lower() in exts]\n",
    "\n",
    "rows = []\n",
    "with ThreadPoolExecutor(max_workers=workers) as ex_pool:\n",
    "    futures = []\n",
    "    for p in files:\n",
    "        rel = p.relative_to(in_root)\n",
    "        futures.append(ex_pool.submit(process_one, p, out_root, rel))\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Preprocessing (fundus → 299)\"):\n",
    "        rows.append(fut.result())\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "labels = []\n",
    "for r in df[\"in\"]:\n",
    "    rel = Path(r).relative_to(in_root)\n",
    "    labels.append(rel.parts[0] if len(rel.parts) > 1 else \"unknown\")\n",
    "df[\"label\"] = labels\n",
    "df.to_csv(out_root / manifest, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Summary:\\n\", df[\"status\"].value_counts())\n",
    "print(f\"\\nSaved images & manifest to: {out_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a1e10-034b-4935-b844-4628fc55d272",
   "metadata": {},
   "source": [
    "## Preprocessing 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d4ada5-a354-4e86-8dea-78e8f40ce0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing (fundus → 384): 100%|████████████████████████████████████████████████| 4728/4728 [04:05<00:00, 19.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " status\n",
      "ok    4728\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved images & manifest to: preprocessed384_best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== SET YOUR PATHS HERE (384 px) ======\n",
    "input_dir  = r\"dataset\"                 # folder with class folders\n",
    "output_dir = r\"preprocessed384_best\"\n",
    "size       = 384                        # <-- target size\n",
    "ext        = \"jpg\"\n",
    "workers    = 8\n",
    "manifest   = \"manifest.csv\"\n",
    "# ==========================================\n",
    "\n",
    "# --- helpers ---\n",
    "def imread_any(path: Path):\n",
    "    arr = np.fromfile(str(path), dtype=np.uint8)\n",
    "    return cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "def imwrite_any(path: Path, img_bgr: np.ndarray, ext=\"jpg\", jpg_quality=95, png_compress=3):\n",
    "    path = path.with_suffix(f\".{ext.lower()}\")\n",
    "    if ext.lower() in (\"jpg\", \"jpeg\"):\n",
    "        ok, enc = cv2.imencode(\".jpg\", img_bgr, [int(cv2.IMWRITE_JPEG_QUALITY), jpg_quality])\n",
    "    elif ext.lower() == \"png\":\n",
    "        ok, enc = cv2.imencode(\".png\", img_bgr, [int(cv2.IMWRITE_PNG_COMPRESSION), png_compress])\n",
    "    else:\n",
    "        ok, enc = cv2.imencode(f\".{ext}\", img_bgr)\n",
    "    if not ok:\n",
    "        return False\n",
    "    enc.tofile(str(path))\n",
    "    return True\n",
    "\n",
    "def robust_bbox_from_mask(mask: np.ndarray, margin_ratio=0.02):\n",
    "    coords = cv2.findNonZero(mask)\n",
    "    if coords is None:\n",
    "        h, w = mask.shape[:2]\n",
    "        return 0, 0, w, h\n",
    "    x, y, w, h = cv2.boundingRect(coords)\n",
    "    m = int(max(h, w) * margin_ratio)\n",
    "    x = max(0, x - m); y = max(0, y - m)\n",
    "    return x, y, min(mask.shape[1] - x, w + 2*m), min(mask.shape[0] - y, h + 2*m)\n",
    "\n",
    "def fundus_roi_crop(bgr: np.ndarray):\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 8, 255, cv2.THRESH_BINARY)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((7,7), np.uint8))\n",
    "    x, y, w, h = robust_bbox_from_mask(mask)\n",
    "    return bgr[y:y+h, x:x+w]\n",
    "\n",
    "def shade_correction(bgr: np.ndarray, sigma=40):\n",
    "    I = bgr.astype(np.float32) + 1.0\n",
    "    bg = cv2.GaussianBlur(I, (0,0), sigmaX=sigma, sigmaY=sigma)\n",
    "    corrected = I / (bg + 1e-6) * 128.0\n",
    "    return np.clip(corrected, 0, 255).astype(np.uint8)\n",
    "\n",
    "def shades_of_gray_cc(bgr: np.ndarray, p=6, eps=1e-6):\n",
    "    I = bgr.astype(np.float32)\n",
    "    Rp = np.power(I[:,:,2], p).mean() ** (1.0/p)\n",
    "    Gp = np.power(I[:,:,1], p).mean() ** (1.0/p)\n",
    "    Bp = np.power(I[:,:,0], p).mean() ** (1.0/p)\n",
    "    scale = (Rp + Gp + Bp) / 3.0\n",
    "    R = I[:,:,2] * (scale / (Rp + eps))\n",
    "    G = I[:,:,1] * (scale / (Gp + eps))\n",
    "    B = I[:,:,0] * (scale / (Bp + eps))\n",
    "    out = np.stack([B, G, R], axis=2)\n",
    "    return np.clip(out, 0, 255).astype(np.uint8)\n",
    "\n",
    "def clahe_on_green(bgr: np.ndarray, clip=2.0, tile=(8,8)):\n",
    "    b, g, r = cv2.split(bgr)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
    "    g2 = clahe.apply(g)\n",
    "    return cv2.merge([b, g2, r])\n",
    "\n",
    "def optional_l_channel_clahe(bgr: np.ndarray, clip=1.5, tile=(8,8)):\n",
    "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
    "    L, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
    "    L2 = clahe.apply(L)\n",
    "    lab2 = cv2.merge([L2, a, b])\n",
    "    return cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def adaptive_gamma(bgr: np.ndarray, target=0.42):\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "    med = np.median(gray) / 255.0\n",
    "    med = np.clip(med, 1e-4, 0.999)\n",
    "    gamma = np.log(target) / np.log(med)\n",
    "    inv = 1.0 / np.clip(gamma, 0.2, 5.0)\n",
    "    lut = np.arange(256, dtype=np.float32) / 255.0\n",
    "    lut = np.power(lut, inv)\n",
    "    lut = np.clip(lut * 255.0, 0, 255).astype(np.uint8)\n",
    "    return cv2.LUT(bgr, lut)\n",
    "\n",
    "def unsharp(bgr: np.ndarray, sigma=1.0, amount=0.5):\n",
    "    blurred = cv2.GaussianBlur(bgr, (0,0), sigmaX=sigma, sigmaY=sigma)\n",
    "    sharp = cv2.addWeighted(bgr, 1 + amount, blurred, -amount, 0)\n",
    "    return np.clip(sharp, 0, 255).astype(np.uint8)\n",
    "\n",
    "def letterbox_square(bgr: np.ndarray, size=384, color=(0,0,0)):\n",
    "    h, w = bgr.shape[:2]\n",
    "    dim = max(h, w)\n",
    "    top = (dim - h) // 2\n",
    "    bottom = dim - h - top\n",
    "    left = (dim - w) // 2\n",
    "    right = dim - w - left\n",
    "    padded = cv2.copyMakeBorder(bgr, top, bottom, left, right,\n",
    "                                cv2.BORDER_CONSTANT, value=color)\n",
    "    # If downscaling a lot, INTER_AREA can be nicer; INTER_CUBIC is fine generally.\n",
    "    return cv2.resize(padded, (size, size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def preprocess_fundus(img_bgr: np.ndarray, size=384):\n",
    "    img = fundus_roi_crop(img_bgr)\n",
    "    img = shade_correction(img, sigma=40)\n",
    "    img = shades_of_gray_cc(img, p=6)\n",
    "    img = clahe_on_green(img, clip=2.0, tile=(8,8))\n",
    "    img = optional_l_channel_clahe(img, clip=1.5, tile=(8,8))\n",
    "    img = adaptive_gamma(img, target=0.42)\n",
    "    img = unsharp(img, sigma=1.0, amount=0.5)\n",
    "    img = letterbox_square(img, size=size, color=(0,0,0))  # -> 384x384\n",
    "    return img\n",
    "\n",
    "def process_one(in_path: Path, out_root: Path, rel: Path):\n",
    "    out_path = (out_root / rel).with_suffix(f\".{ext}\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": \"skipped\"}\n",
    "    bgr = imread_any(in_path)\n",
    "    if bgr is None:\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": \"read_error\"}\n",
    "    try:\n",
    "        proc = preprocess_fundus(bgr, size=size)\n",
    "        ok = imwrite_any(out_path, proc, ext=ext)\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": \"ok\" if ok else \"write_error\"}\n",
    "    except Exception as e:\n",
    "        return {\"in\": str(in_path), \"out\": str(out_path), \"status\": f\"error:{e}\"}\n",
    "\n",
    "# --- run preprocessing ---\n",
    "in_root = Path(input_dir)\n",
    "out_root = Path(output_dir)\n",
    "out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "files = [p for p in in_root.rglob(\"*\") if p.suffix.lower() in exts]\n",
    "\n",
    "rows = []\n",
    "with ThreadPoolExecutor(max_workers=workers) as ex_pool:\n",
    "    futures = []\n",
    "    for p in files:\n",
    "        rel = p.relative_to(in_root)\n",
    "        futures.append(ex_pool.submit(process_one, p, out_root, rel))\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Preprocessing (fundus → 384)\"):\n",
    "        rows.append(fut.result())\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "labels = []\n",
    "for r in df[\"in\"]:\n",
    "    rel = Path(r).relative_to(in_root)\n",
    "    labels.append(rel.parts[0] if len(rel.parts) > 1 else \"unknown\")\n",
    "df[\"label\"] = labels\n",
    "df.to_csv(out_root / manifest, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Summary:\\n\", df[\"status\"].value_counts())\n",
    "print(f\"\\nSaved images & manifest to: {out_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b7a2e-0662-4285-8294-84654e3419b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
